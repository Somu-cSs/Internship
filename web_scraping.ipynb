{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# C SOMA SEKHAR\n",
        "  \n",
        "  **BATCH : DS2307**\n",
        "\n",
        "**WEB SCRAPING**"
      ],
      "metadata": {
        "id": "OHEj9Qjny4Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1.Python program to display all the header tags from wikipedia.org and make data frame.\n"
      ],
      "metadata": {
        "id": "hICC4NHu_RkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Get URL\n",
        "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
        "\n",
        "# Scrape webpage\n",
        "soup = BeautifulSoup(page.content, 'lxml')\n",
        "\n",
        "heading_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
        "headers_data = []\n",
        "\n",
        "for tag in soup.find_all(heading_tags):\n",
        "    headers_data.append({'Tag': tag.name,'Text': tag.text.strip() })\n",
        "\n",
        "headers_df = pd.DataFrame(headers_data)\n",
        "print(headers_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdN0i-RVAwKg",
        "outputId": "f65814ee-482a-45c1-b7c1-a144bdb9d477"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Tag                           Text\n",
            "0  h1                      Main Page\n",
            "1  h1           Welcome to Wikipedia\n",
            "2  h2  From today's featured article\n",
            "3  h2               Did you knowÂ ...\n",
            "4  h2                    In the news\n",
            "5  h2                    On this day\n",
            "6  h2       Today's featured picture\n",
            "7  h2       Other areas of Wikipedia\n",
            "8  h2    Wikipedia's sister projects\n",
            "9  h2            Wikipedia languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3.A python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
        "a) Top 10 ODI teams in mens cricket along with the records for matches, points and rating.\n",
        "\n",
        "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
        "\n",
        "c) Top 10 ODI bowlers along with the records of their team andrating"
      ],
      "metadata": {
        "id": "3lWVoO1J_gL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to scrape rankings\n",
        "def scrape_rankings(url, num_players):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    players_data = []\n",
        "\n",
        "    # Find the table containing the rankings\n",
        "    table = soup.find(\"table\", class_=\"table\")\n",
        "\n",
        "    if table:\n",
        "        rows = table.find_all(\"tr\")\n",
        "        for row in rows[1:num_players+1]:  # Skip the header and get top players\n",
        "            cols = row.find_all(\"td\")\n",
        "\n",
        "            # Extract player name and team\n",
        "            player_elem = cols[1].find(\"div\", class_=\"rankings-player__name\")\n",
        "            player_name = player_elem.text.strip() if player_elem else \"\"\n",
        "\n",
        "            team_elem = cols[1].find(\"div\", class_=\"rankings-player__nationality\")\n",
        "            team = team_elem.text.strip() if team_elem else \"\"\n",
        "\n",
        "            rating = cols[3].text.strip()\n",
        "\n",
        "            players_data.append([player_name, team, rating])\n",
        "\n",
        "    return players_data\n",
        "\n",
        "# Scrape top 10 ODI teams in women's cricket\n",
        "teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
        "teams_data = scrape_rankings(teams_url, num_players=10)\n",
        "teams_df = pd.DataFrame(teams_data, columns=[\"Team\", \"Matches\", \"Points\"])\n",
        "\n",
        "# Scrape top 10 women's ODI batting players\n",
        "batting_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
        "batting_data = scrape_rankings(batting_url, num_players=10)\n",
        "batting_df = pd.DataFrame(batting_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
        "\n",
        "# Scrape top 10 women's ODI all-rounders\n",
        "allrounders_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/all-rounder\"\n",
        "allrounders_data = scrape_rankings(allrounders_url, num_players=10)\n",
        "allrounders_df = pd.DataFrame(allrounders_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
        "\n",
        "# Print DataFrames\n",
        "print(\"Top 10 ODI Teams:\")\n",
        "print(teams_df)\n",
        "print(\"\\nTop 10 men's ODI Batting Players:\")\n",
        "print(batting_df)\n",
        "print(\"\\nTop 10 men's ODI All-rounders:\")\n",
        "print(allrounders_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW4cJAmUxLIa",
        "outputId": "c1693d80-468b-4e24-8cbd-cb9851502c2b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 ODI Teams:\n",
            "  Team Matches Points\n",
            "0               2,714\n",
            "1               2,316\n",
            "2               4,081\n",
            "3               2,806\n",
            "4               2,426\n",
            "5               1,910\n",
            "6               2,661\n",
            "7               1,404\n",
            "8               2,794\n",
            "9               2,582\n",
            "\n",
            "Top 10 men's ODI Batting Players:\n",
            "  Player Team Rating\n",
            "0                886\n",
            "1                777\n",
            "2                755\n",
            "3                745\n",
            "4                743\n",
            "5                726\n",
            "6                726\n",
            "7                718\n",
            "8                705\n",
            "9                702\n",
            "\n",
            "Top 10 men's ODI All-rounders:\n",
            "  Player Team Rating\n",
            "0                371\n",
            "1                298\n",
            "2                287\n",
            "3                272\n",
            "4                248\n",
            "5                235\n",
            "6                234\n",
            "7                233\n",
            "8                228\n",
            "9                215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. A python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
        "a) Top 10 ODI teams in womens cricket along with the records for matches, points and rating.\n",
        "\n",
        "b) Top 10 womens ODI Batting players along with the records of their team and rating.\n",
        "\n",
        "c) Top 10 womens ODI all-rounder along with the records of their team and rating."
      ],
      "metadata": {
        "id": "pb6O7hvSAVpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to scrape rankings\n",
        "def scrape_rankings(url, num_players):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    players_data = []\n",
        "\n",
        "    # Find the table containing the rankings\n",
        "    table = soup.find(\"table\", class_=\"table\")\n",
        "\n",
        "    if table:\n",
        "        rows = table.find_all(\"tr\")\n",
        "        for row in rows[1:num_players+1]:  # Skip the header and get top players\n",
        "            cols = row.find_all(\"td\")\n",
        "\n",
        "            # Extract player name and team\n",
        "            player_elem = cols[1].find(\"div\", class_=\"rankings-player__name\")\n",
        "            player_name = player_elem.text.strip() if player_elem else \"\"\n",
        "\n",
        "            team_elem = cols[1].find(\"div\", class_=\"rankings-player__nationality\")\n",
        "            team = team_elem.text.strip() if team_elem else \"\"\n",
        "\n",
        "            rating = cols[3].text.strip()\n",
        "\n",
        "            players_data.append([player_name, team, rating])\n",
        "\n",
        "    return players_data\n",
        "\n",
        "# Scrape top 10 ODI teams in women's cricket\n",
        "teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
        "teams_data = scrape_rankings(teams_url, num_players=10)\n",
        "teams_df = pd.DataFrame(teams_data, columns=[\"Team\", \"Matches\", \"Points\"])\n",
        "\n",
        "# Scrape top 10 women's ODI batting players\n",
        "batting_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
        "batting_data = scrape_rankings(batting_url, num_players=10)\n",
        "batting_df = pd.DataFrame(batting_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
        "\n",
        "# Scrape top 10 women's ODI all-rounders\n",
        "allrounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
        "allrounders_data = scrape_rankings(allrounders_url, num_players=10)\n",
        "allrounders_df = pd.DataFrame(allrounders_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
        "\n",
        "# Print DataFrames\n",
        "print(\"Top 10 ODI Teams:\")\n",
        "print(teams_df)\n",
        "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
        "print(batting_df)\n",
        "print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
        "print(allrounders_df)\n"
      ],
      "metadata": {
        "id": "W7utL7O-AmqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e57499bf-2b19-46b4-9a52-92c8122dc094"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 ODI Teams:\n",
            "  Team Matches Points\n",
            "0               4,290\n",
            "1               3,875\n",
            "2               3,098\n",
            "3               3,039\n",
            "4               2,688\n",
            "5               2,743\n",
            "6               1,284\n",
            "7                 820\n",
            "8                 883\n",
            "9               1,678\n",
            "\n",
            "Top 10 Women's ODI Batting Players:\n",
            "  Player Team Rating\n",
            "0                803\n",
            "1                758\n",
            "2                751\n",
            "3                732\n",
            "4                708\n",
            "5                702\n",
            "6                694\n",
            "7                686\n",
            "8                682\n",
            "9                618\n",
            "\n",
            "Top 10 Women's ODI All-rounders:\n",
            "  Player Team Rating\n",
            "0                421\n",
            "1                389\n",
            "2                382\n",
            "3                349\n",
            "4                329\n",
            "5                328\n",
            "6                312\n",
            "7                241\n",
            "8                233\n",
            "9                232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. A python program to scrape mentioned news details from\n",
        " https://www.cnbc.com/world/?region=world\n",
        "\n",
        "and make data frame\n",
        "\n",
        "i) Headline\n",
        "ii) Time\n",
        "iii) News Link"
      ],
      "metadata": {
        "id": "so0fEH8PAw6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://www.cnbc.com/world/?region=world\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "headlines = []\n",
        "times = []\n",
        "news_links = []\n",
        "\n",
        "news_cards = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
        "\n",
        "for card in news_cards:\n",
        "    # Headline\n",
        "    headline_elem = card.find(\"a\", class_=\"Card-titleLink\")\n",
        "    headline = headline_elem.text.strip() if headline_elem else \"\"\n",
        "    headlines.append(headline)\n",
        "\n",
        "    # Time\n",
        "    time_elem = card.find(\"time\")\n",
        "    time = time_elem.text.strip() if time_elem else \"\"\n",
        "    times.append(time)\n",
        "\n",
        "    # News Link\n",
        "    news_link = \"https://www.cnbc.com\" + headline_elem[\"href\"] if headline_elem and \"href\" in headline_elem.attrs else \"\"\n",
        "    news_links.append(news_link)\n",
        "\n",
        "data = {\n",
        "    \"Headline\": headlines,\n",
        "    \"Time\": times,\n",
        "    \"News Link\": news_links\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7HM271vr6kY",
        "outputId": "ff641f84-60f1-46ac-a971-0a5558a26f46"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Headline Time News Link\n",
            "0                         \n",
            "1                         \n",
            "2                         \n",
            "3                         \n",
            "4                         \n",
            "5                         \n",
            "6                         \n",
            "7                         \n",
            "8                         \n",
            "9                         \n",
            "10                        \n",
            "11                        \n",
            "12                        \n",
            "13                        \n",
            "14                        \n",
            "15                        \n",
            "16                        \n",
            "17                        \n",
            "18                        \n",
            "19                        \n",
            "20                        \n",
            "21                        \n",
            "22                        \n",
            "23                        \n",
            "24                        \n",
            "25                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6.A python program to scrape the details of most downloaded articles from AI in last 90days.\n",
        "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
        "\n",
        "Scrape below mentioned details and make data frame\n",
        "\n",
        "i) Paper Title\n",
        "ii) Authors\n",
        "iii) Published Date\n",
        "iv) Paper URL\n"
      ],
      "metadata": {
        "id": "R33bQvy6BS7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "paper_titles = []\n",
        "authors_list = []\n",
        "published_dates = []\n",
        "paper_urls = []\n",
        "\n",
        "paper_cards = soup.find_all(\"li\", class_=\"pod-list-item\")\n",
        "\n",
        "for card in paper_cards:\n",
        "    # Paper Title\n",
        "    title_elem = card.find(\"h2\", class_=\"pod-list-header\")\n",
        "    paper_title = title_elem.text.strip() if title_elem else \"\"\n",
        "    paper_titles.append(paper_title)\n",
        "\n",
        "    # Authors\n",
        "    authors_elem = card.find(\"ul\", class_=\"authors\")\n",
        "    authors = [author.text.strip() for author in authors_elem.find_all(\"li\")] if authors_elem else []\n",
        "    authors_list.append(authors)\n",
        "\n",
        "    # Published Date\n",
        "    date_elem = card.find(\"div\", class_=\"pod-list-footer\").find(\"span\", class_=\"date\")\n",
        "    published_date = date_elem.text.strip() if date_elem else \"\"\n",
        "    published_dates.append(published_date)\n",
        "\n",
        "    # Paper URL\n",
        "    link_elem = card.find(\"h2\", class_=\"pod-list-header\").find(\"a\")\n",
        "    paper_url = link_elem[\"href\"] if link_elem and \"href\" in link_elem.attrs else \"\"\n",
        "    paper_urls.append(paper_url)\n",
        "\n",
        "data = {\n",
        "    \"Paper Title\": paper_titles,\n",
        "    \"Authors\": authors_list,\n",
        "    \"Published Date\": published_dates,\n",
        "    \"Paper URL\": paper_urls\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCtcRjOxrHPB",
        "outputId": "42e13cd9-c6e8-4bd4-a7a9-37ba469a4480"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7.A python program to scrape mentioned details from\n",
        "https://www.dineout.co.in/\n",
        "\n",
        "and make data frame.\n",
        "\n",
        "i) Restaurant name\n",
        "ii) Cuisine\n",
        "iii) Location\n",
        "iv) Ratings\n",
        "v) Image UR"
      ],
      "metadata": {
        "id": "N0tKR_FfBrzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "restaurant_names = []\n",
        "cuisines = []\n",
        "locations = []\n",
        "ratings = []\n",
        "image_urls = []\n",
        "\n",
        "restaurant_cards = soup.find_all(\"div\", class_=\"restnt-card\")\n",
        "\n",
        "for card in restaurant_cards:\n",
        "    # Restaurant name\n",
        "    restaurant_name_elem = card.find(\"div\", class_=\"restnt-name\")\n",
        "    if restaurant_name_elem:\n",
        "        restaurant_name = restaurant_name_elem.get_text(strip=True)\n",
        "    else:\n",
        "        restaurant_name = \"\"\n",
        "    restaurant_names.append(restaurant_name)\n",
        "\n",
        "    # Cuisine\n",
        "    cuisine_elem = card.find(\"span\", class_=\"double-line-ellipsis\")\n",
        "    if cuisine_elem:\n",
        "        cuisine = cuisine_elem.get_text(strip=True)\n",
        "    else:\n",
        "        cuisine = \"\"\n",
        "    cuisines.append(cuisine)\n",
        "\n",
        "    # Location\n",
        "    location_elem = card.find(\"div\", class_=\"restnt-loc\")\n",
        "    if location_elem:\n",
        "        location = location_elem.get_text(strip=True)\n",
        "    else:\n",
        "        location = \"\"\n",
        "    locations.append(location)\n",
        "\n",
        "    # Ratings\n",
        "    rating_elem = card.find(\"div\", class_=\"restnt-rating\")\n",
        "    if rating_elem:\n",
        "        rating = rating_elem.get_text(strip=True)\n",
        "    else:\n",
        "        rating = \"\"\n",
        "    ratings.append(rating)\n",
        "\n",
        "    # Image URL\n",
        "    image_elem = card.find(\"img\", class_=\"lazy\")\n",
        "    if image_elem:\n",
        "        image_url = image_elem.get(\"data-src\", \"\")\n",
        "    else:\n",
        "        image_url = \"\"\n",
        "    image_urls.append(image_url)\n",
        "\n",
        "data = {\n",
        "    \"Restaurant Name\": restaurant_names,\n",
        "    \"Cuisine\": cuisines,\n",
        "    \"Location\": locations,\n",
        "    \"Ratings\": ratings,\n",
        "    \"Image URL\": image_urls\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqnIZQzqqytb",
        "outputId": "d2a0fe2f-c2bf-470e-e9c3-7a0d5563e5ad"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Restaurant Name                                            Cuisine  \\\n",
            "0                   â¹ 1,100 for 2 (approx)|Italian,Chinese,North I...   \n",
            "1                   â¹ 2,000 for 2 (approx)|North Indian,Asian,Cont...   \n",
            "2                   â¹ 2,000 for 2 (approx)|Continental,Asian,Itali...   \n",
            "3                   â¹ 2,000 for 2 (approx)|North Indian,Asian,Italian   \n",
            "4                   â¹ 2,000 for 2 (approx)|Finger Food,Chinese,Con...   \n",
            "5                                 â¹ 2,000 for 2 (approx)|North Indian   \n",
            "6                   â¹ 2,500 for 2 (approx)|North Indian,Chinese,It...   \n",
            "7                   â¹ 2,100 for 2 (approx)|North Indian,Continenta...   \n",
            "8                   â¹ 3,000 for 2 (approx)|North Indian,Continenta...   \n",
            "9                   â¹ 1,800 for 2 (approx)|North Indian,Continenta...   \n",
            "10                  â¹ 2,500 for 2 (approx)|North Indian,Italian,Ch...   \n",
            "11                                â¹ 1,300 for 2 (approx)|Chinese,Thai   \n",
            "12                  â¹ 2,500 for 2 (approx)|Chinese,North Indian,Fa...   \n",
            "13                  â¹ 800 for 2 (approx)|North Indian,South Indian...   \n",
            "14                        â¹ 1,500 for 2 (approx)|North Indian,Chinese   \n",
            "15                    â¹ 1,400 for 2 (approx)|North Indian,Continental   \n",
            "16                  â¹ 1,100 for 2 (approx)|Continental,North India...   \n",
            "17                  â¹ 2,100 for 2 (approx)|North Indian,Continenta...   \n",
            "18                  â¹ 2,700 for 2 (approx)|North Indian,Chinese,Co...   \n",
            "19                  â¹ 2,000 for 2 (approx)|North Indian,Chinese,It...   \n",
            "20                  â¹ 2,600 for 2 (approx)|North Indian,Mughlai,Ch...   \n",
            "\n",
            "                                       Location Ratings Image URL  \n",
            "0         F-Block,Connaught Place,Central Delhi       4            \n",
            "1   Scindia House,Connaught Place,Central Delhi       4            \n",
            "2                 Connaught Place,Central Delhi     4.2            \n",
            "3                 Connaught Place,Central Delhi     4.1            \n",
            "4                 Connaught Place,Central Delhi     3.9            \n",
            "5         M-Block,Connaught Place,Central Delhi     4.3            \n",
            "6                 Connaught Place,Central Delhi     4.1            \n",
            "7                 Connaught Place,Central Delhi     4.1            \n",
            "8         M-Block,Connaught Place,Central Delhi       4            \n",
            "9                 Connaught Place,Central Delhi     4.2            \n",
            "10                Connaught Place,Central Delhi       4            \n",
            "11                Connaught Place,Central Delhi     4.3            \n",
            "12                Connaught Place,Central Delhi     4.2            \n",
            "13                Connaught Place,Central Delhi     4.2            \n",
            "14                Connaught Place,Central Delhi       4            \n",
            "15                Connaught Place,Central Delhi       4            \n",
            "16        F-Block,Connaught Place,Central Delhi       4            \n",
            "17                Connaught Place,Central Delhi     4.2            \n",
            "18        M-Block,Connaught Place,Central Delhi     4.3            \n",
            "19                Connaught Place,Central Delhi     4.1            \n",
            "20                Connaught Place,Central Delhi     4.1            \n"
          ]
        }
      ]
    }
  ]
}